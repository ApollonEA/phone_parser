{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa1790b9-0c1f-4be0-a544-597e853bf4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type the name of industry pech_dlya_bani\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SuperCum UAV\n",
      "started working with http://077.ru\n",
      "started working with http://100-kpd.ru\n",
      "started working with http://1000pechi.ru\n",
      "started working with http://100pechei.ru\n",
      "started working with http://1phenix.ru\n",
      "started working with http://24-ok.ru\n",
      "started working with http://24kwt.ru\n",
      "started working with http://33sm.ru\n",
      "started working with http://46saun.ru\n",
      "started working with http://812banya.ru\n",
      "started working with http://akademiyasvarki.ru\n",
      "started working with http://almazmet.ru\n",
      "started working with http://amarket.online\n",
      "started working with http://antonovsad.ru\n",
      "started working with http://art-kaminspb.ru\n",
      "started working with http://astmarket.com\n",
      "started working with http://ateliesaun.ru\n",
      "started working with http://avangard2003.ru\n",
      "started working with http://banbas-nn.com\n",
      "started working with http://bani-v-spb.ru\n",
      "started working with http://baniglushakova.ru\n",
      "started working with http://banisauny-tver.ru\n",
      "started working with http://bannik.ru\n",
      "started working with http://banpechi.ru\n",
      "started working with http://bany-bochki-24.ru\n",
      "started working with http://banya-bochka.su\n",
      "started working with http://banyaklass.ru\n",
      "started working with http://banyaprosto.ru\n",
      "started working with http://beribanyu.ru\n",
      "started working with http://bochky.ru\n",
      "started working with http://breneran.ru\n",
      "started working with http://c-s-k.ru\n",
      "started working with http://chestroy.ru\n",
      "started working with http://climate-group.ru\n",
      "started working with http://compromesso.ru\n",
      "started working with http://cooperwood.ru\n",
      "started working with http://d-d-i.ru\n",
      "started working with http://dacha2000.ru\n",
      "started working with http://dachny.expert\n",
      "started working with http://derevovdom.ru\n",
      "started working with http://dkedra.ru\n",
      "started working with http://dns-shop.ru\n",
      "started working with http://dobrostal.ru\n",
      "started working with http://domclick.ru\n",
      "started working with http://dompechei.ru\n",
      "started working with http://dongradus.ru\n",
      "started working with http://drevolog.ru\n",
      "Connection Error: Exceeded 30 redirects.\n",
      "started working with http://dymohod-pech.ru\n",
      "started working with http://dzen.ru\n",
      "started working with http://ecokamin.ru\n",
      "started working with http://ecotime-market.ru\n",
      "started working with http://elekon-online.ru\n",
      "started working with http://elitdom-ufa.ru\n",
      "started working with http://euroles.su\n",
      "started working with http://europechi.ru\n",
      "started working with http://evrodom-rd.ru\n",
      "started working with http://evrokamin.ru\n",
      "started working with http://explorer-russia.ru\n",
      "started working with http://feringer.ru\n",
      "started working with http://feringermsk.ru\n",
      "started working with http://fornaks.ru\n",
      "started working with http://forpost-sevastopol.ru\n",
      "started working with http://geizer-kzn.ru\n",
      "started working with http://gipernn.ru\n",
      "started working with http://glavpech.ru\n",
      "started working with http://goodster.ru\n",
      "started working with http://gornilo.ru\n",
      "started working with http://gorod-masterov.com\n",
      "started working with http://gzs67.ru\n",
      "started working with http://houzz.ru\n",
      "started working with http://hozstroymag.ru\n",
      "started working with http://ilnar-kamin.ru\n",
      "started working with http://jsprav.ru\n",
      "started working with http://kamenca.ru\n",
      "started working with http://kamenka.ru\n",
      "started working with http://kamin-expert.com\n",
      "started working with http://kamin-mos.ru\n",
      "started working with http://kamin-sale.ru\n",
      "started working with http://kamin-tm.ru\n",
      "started working with http://kamin.ru\n",
      "started working with http://kamin1.com\n",
      "started working with http://kamin53.ru\n",
      "started working with http://kamin74.ru\n",
      "started working with http://kaminbryansk.ru\n",
      "started working with http://kamindom.ru\n",
      "started working with http://kaminplanet.ru\n",
      "started working with http://kaminvdom.ru\n",
      "started working with http://karpov52.ru\n",
      "started working with http://katalogtepla.ru\n",
      "started working with http://klimat-pro.ru\n",
      "started working with http://knr24.ru\n",
      "started working with http://kotel-nn.ru\n",
      "started working with http://kovka-line.com\n",
      "started working with http://kpd-pechi.ru\n",
      "started working with http://lancmanschool.ru\n",
      "started working with http://le-magasin.ru\n",
      "started working with http://leroymerlin.ru\n",
      "started working with http://lipa74.ru\n",
      "started working with http://lit-kom.ru\n",
      "started working with http://liteikapro.ru\n",
      "started working with http://lst-vrn.ru\n",
      "started working with http://m-kamin.ru\n",
      "started working with http://mag-pechi.ru\n",
      "started working with http://magazinpechi.ru\n",
      "started working with http://master-sauna.ru\n",
      "started working with http://mbt-ug.ru\n",
      "started working with http://megapech.ru\n",
      "started working with http://megapolys.com\n",
      "started working with http://megastroy.com\n",
      "started working with http://mestam.info\n",
      "started working with http://metallbaza.ru\n",
      "started working with http://mir-kaminov.su\n",
      "started working with http://mirf.ru\n",
      "started working with http://mirpechek.ru\n",
      "started working with http://mirsaun-nn.ru\n",
      "started working with http://my-systems.ru\n",
      "started working with http://my-teplo.ru\n",
      "started working with http://myfin.by\n",
      "started working with http://n-dom.com\n",
      "started working with http://nagaevo-stroy.ru\n",
      "started working with http://nashural.ru\n",
      "started working with http://niann.ru\n",
      "started working with http://nkamin.ru\n",
      "started working with http://nn.ru\n",
      "started working with http://nnkamin.ru\n",
      "started working with http://novgorodskayaizba.ru\n",
      "started working with http://ochag-rnd.ru\n",
      "started working with http://ochag-ufa.ru\n",
      "started working with http://ochag-vrn.ru\n",
      "started working with http://ogonek-kzn.ru\n",
      "started working with http://ogonpechi.ru\n",
      "started working with http://ok-stroy.ru\n",
      "started working with http://order-opt.ru\n",
      "started working with http://otoplenie-online.ru\n",
      "started working with http://ozinkovka.ru\n",
      "started working with http://parilkaspb.ru\n",
      "started working with http://pban.ru\n",
      "Connection Error: HTTPConnectionPool(host='pe4ikamini.ru', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000161B8FE5FC0>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed'))\n",
      "started working with http://pech-banya.ru\n",
      "started working with http://pech-berezka.ru\n",
      "started working with http://pech-center.ru\n",
      "started working with http://pech.ru\n",
      "started working with http://pechdvor.ru\n",
      "started working with http://pechgrad.ru\n",
      "started working with http://pechi-dimohody.ru\n",
      "started working with http://pechi-discont.ru\n",
      "started working with http://pechi-kirov.ru\n",
      "started working with http://pechi-online.ru\n",
      "started working with http://pechi-opt.ru\n",
      "started working with http://pechi-rostov.ru\n",
      "started working with http://pechi-sauna.ru\n",
      "started working with http://pechi-troyka.ru\n",
      "started working with http://pechi-tut.ru\n",
      "started working with http://pechi102.ru\n",
      "started working with http://pechibani.com\n",
      "started working with http://pechibreneran.ru\n",
      "started working with http://pechiikamini.ru\n",
      "started working with http://pechilux.ru\n",
      "started working with http://pechimax.ru\n",
      "started working with http://pechiteplov.ru\n",
      "started working with http://pechiv.ru\n",
      "started working with http://pechivrn.ru\n",
      "started working with http://pechki66.ru\n",
      "started working with http://pechmir.ru\n",
      "started working with http://pechnik-rzn.ru\n",
      "started working with http://pechnik.su\n",
      "started working with http://pechnik74.ru\n",
      "started working with http://pechnikov-kzn.ru\n",
      "started working with http://pechnoi-centr52.ru\n",
      "started working with http://pechnoy.expert\n",
      "started working with http://pechnoydom.ru\n",
      "started working with http://pechy-kaminy.ru\n",
      "started working with http://petrovich.ru\n",
      "started working with http://piter-kamin.ru\n",
      "started working with http://planetapechey.ru\n",
      "started working with http://poldoma-nn.ru\n",
      "started working with http://priceok.ru\n",
      "started working with http://pro-kaminy.ru\n",
      "started working with http://pro-komfort.com\n",
      "Connection Error: HTTPConnectionPool(host='prom-katalog.ru', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000161B8FE73A0>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed'))\n",
      "Connection Error: HTTPConnectionPool(host='prometeyspb.ru', port=80): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000161B8FE53C0>, 'Connection to prometeyspb.ru timed out. (connect timeout=30)'))\n",
      "started working with http://promindex.ru\n",
      "started working with http://protopka.su\n",
      "started working with http://pskov-bani.ru\n",
      "started working with http://r-pechi.ru\n",
      "started working with http://realnoevremya.ru\n",
      "started working with http://regtorg.ru\n",
      "started working with http://remontdoma24.ru\n",
      "started working with http://remotvet.ru\n",
      "started working with http://reselektrokotel.ru\n",
      "started working with http://rom-stroy.ru\n",
      "started working with http://ros-spravka.ru\n",
      "started working with http://rosndv.ru\n",
      "started working with http://rospechtorg.ru\n",
      "started working with http://rostov-dymohod.ru\n",
      "started working with http://rubibani.ru\n",
      "started working with http://rupar.ru\n",
      "started working with http://rusart-opt.ru\n",
      "started working with http://rusles-35.ru\n",
      "started working with http://saintbor.ru\n",
      "started working with http://santehnika-v-rostove.ru\n",
      "Connection Error: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))\n",
      "started working with http://satom.ru\n",
      "started working with http://sauna-nn.ru\n",
      "started working with http://saunacityufa.ru\n",
      "started working with http://saunamart.ru\n",
      "started working with http://saunavrn.ru\n",
      "started working with http://sbermegamarket.ru\n",
      "started working with http://sdom36.ru\n",
      "started working with http://sharewood.co\n",
      "started working with http://sibirskie-pechi.ru\n",
      "started working with http://sk-bani.ru\n",
      "started working with http://smolkirpich.ru\n",
      "started working with http://sovdom.com\n",
      "started working with http://spb.ru\n",
      "started working with http://sportcity74.ru\n",
      "started working with http://spravka003.ru\n",
      "started working with http://spravkaru.info\n",
      "started working with http://srubvrn.ru\n",
      "started working with http://stanc.ru\n",
      "Connection Error: HTTPSConnectionPool(host='stmgroup.ru', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))\n",
      "started working with http://stroimaterial-nn.ru\n",
      "started working with http://stroitelnye-materialy-v-bryanske.ru\n",
      "started working with http://stroitelnye-materialy-v-kurske.ru\n",
      "Connection Error: HTTPSConnectionPool(host='volgograd.stroy-s.org', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000161B8FC23B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "started working with http://stroylandiya.ru\n",
      "started working with http://stroyoborudovanie74.ru\n",
      "started working with http://stroyport.su\n",
      "started working with http://superpechi.ru\n",
      "started working with http://svarka35.ru\n",
      "started working with http://svarkaperm.ru\n",
      "started working with http://sweetfire.ru\n",
      "started working with http://t-m-f.ru\n",
      "started working with http://tatpool.ru\n",
      "started working with http://td-jupiter.ru\n",
      "started working with http://td-kvartal.ru\n",
      "started working with http://td-prometey.ru\n",
      "started working with http://td102.ru\n",
      "started working with http://td32.ru\n",
      "started working with http://technovel.ru\n",
      "started working with http://tehnoslon.com\n",
      "started working with http://teplavoz.ru\n",
      "started working with http://teplo152.ru\n",
      "started working with http://teplo16.ru\n",
      "started working with http://teplodar.ru\n",
      "started working with http://teplomarket-m.ru\n",
      "started working with http://teplomirnn.ru\n",
      "started working with http://teplotek-ug.ru\n",
      "started working with http://teplotoriya.ru\n",
      "started working with http://teplovdom.net\n",
      "started working with http://teplozhar.ru\n",
      "started working with http://termofort.ru\n",
      "started working with http://termosfera.su\n",
      "started working with http://tk-best.ru\n",
      "started working with http://tk31.su\n",
      "started working with http://tmf-market.ru\n",
      "started working with http://tovaryplus.ru\n",
      "started working with http://tpes.ru\n",
      "started working with http://tssk.pro\n",
      "Connection Error: HTTPSConnectionPool(host='tvoy-usadba.ru', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: DH_KEY_TOO_SMALL] dh key too small (_ssl.c:997)')))\n",
      "started working with http://ufa-kamin.ru\n",
      "started working with http://vashklimat.com\n",
      "started working with http://vimos.ru\n",
      "started working with http://vse-pechi.ru\n",
      "started working with http://vtc43.ru\n",
      "started working with http://vvd.su\n",
      "started working with http://wikipedia.org\n",
      "started working with http://wood-brus.ru\n",
      "started working with http://woodson.ru\n",
      "started working with http://yadachnik.ru\n",
      "started working with http://zagorodshop.ru\n",
      "started working with http://zavodtek.ru\n",
      "started working with http://бани-курск.рф\n",
      "started working with http://банибочки.рф\n",
      "started working with http://банибочкикупить.рф\n",
      "started working with http://вагонка27.рф\n",
      "started working with http://вбрусе.рф\n",
      "started working with http://водолей.рф\n",
      "started working with http://граф-печнов.рф\n",
      "started working with http://дымоход52.рф\n",
      "started working with http://империябань.рф\n",
      "started working with http://карпов52.рф\n",
      "started working with http://котловцентр.рф\n",
      "started working with http://лечьнапечь.рф\n",
      "started working with http://металлоизделия53.рф\n",
      "started working with http://нижегородский-печной-центр.рф\n",
      "started working with http://папакарло.net\n",
      "started working with http://печи-нн.рф\n",
      "started working with http://печи35.рф\n",
      "started working with http://печи52.рф\n",
      "started working with http://печман.рф\n",
      "started working with http://печназ.рф\n",
      "started working with http://печнаялавка.рф\n",
      "started working with http://проект-бани.рф\n",
      "started working with http://реконстрой.рф\n",
      "started working with http://сварка-питер.рф\n",
      "started working with http://сварка35.рф\n",
      "started working with http://стоказанов.рф\n",
      "started working with http://территория-бани.рф\n",
      "started working with http://чудо-баня.рф\n",
      "started working with http://чудобаня.рф\n",
      "Results saved to results_pech_dlya_bani.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "ask user for the name of industry\n",
    "get list of html files\n",
    "get list of urls\n",
    "remove cities from url\n",
    "remore bad sites\n",
    "\n",
    "parse main pages of websites from list of urls \n",
    "for each url find \n",
    "    telephone numbers\n",
    "    ИНН (is or not)\n",
    "    ОРГН (is or not)\n",
    "    ООО (is or not)\n",
    "    ИП (is or not)\n",
    "and for each telephone of each url create a list of  - url - ИНН - ОГРН - ООО - ИП - Telephones in a format of whatsapp link\n",
    "then save it to csv\n",
    "and then manually remove [ ] ' \" with Nitepad++\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "###main\n",
    "\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "from urllib.parse import urlparse\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_of_html_files = []\n",
    "list_of_urls = []\n",
    "list_of_classes_with_urls = []\n",
    "list_of_urls_with_www = []\n",
    "list_of_urls_without_www_of_cities = []\n",
    "result_list_of_urls = []\n",
    "\n",
    "name_of_industry = ''\n",
    "\n",
    "list_of_agreg_sites = ['allix.ru','avito.ru','bazarpnz.ru','bazarsng.ru','dorus.ru','irr.ru','miltor.ru','on-doski.ru','pirpo.ru','rudos.ru','1000dosok.ru',\n",
    "                       'бесплатныеобъявления', 'pulscen','barahla','orgsinfo','uslugi','2417.pro','abtorg.ru','acoola.ru','akula.shop','allix.ru','alvengo.ru',\n",
    "                       'avada.shop','avelango.com','avito.ru','barahla.net','bazarpnz.ru','bazarsng.ru','biz-market.ru','bulboard.ru','cmlt.ru','deilo.ru',\n",
    "                       'delonet.ru','diesel.world','dorus.ru','doska.ru','doski.ru','flagma.ru','irr.ru','ivik.ru','kaluga.sale','kupiprodai.ru','megadoski.ru',\n",
    "                       'miltor.ru','moyareklama.ru','nadoske.info','on-doski.ru','pirpo.ru','prodam-rf.ru','reslando.ru','roodls.net','rudos.ru','sindom.ru',\n",
    "                       'trasto.ru','youla.ru','объявления.рф','объявления.рф'  '1000dosok.ru', '2gis', 'EDC.SALE', 'aport', 'asktel.ru', 'avito', 'barahla.net', \n",
    "                       'blizko', 'bulboard', 'castorama', 'cataloxy', 'farpost.ru', 'fis.ru', 'gdeprosto', 'google', 'hh', 'irr.ru', 'kupiprodai', 'kupiprodai.ru', \n",
    "                       'livingjoy', 'mail.ru', 'mk.ru', 'moyareklama', 'nashaspravka', 'okigo.ru', 'optlist', 'otzovik', 'ozon', 'poisk', 'productcenter', \n",
    "                       'propartner', 'regmarkets', 'rydo.ru', 'skidkom', 'spark-interfax', 'spravker', 'stroyportal', 'tdsot.ru', 'tuttovary', 'vdoskusvoi', \n",
    "                       'vk.com', 'vseinstrumenti', 'wb', 'wildberries', 'yandex', 'yell.ru', 'youla', 'zoon', '220-volt.ru', '2map.su', '4geo.ru', 'abc.ru', \n",
    "                       'abnews.ru', 'al32.ru', 'all-gorod.ru', 'allinform.ru', 'aurora.ru', 'bagra.ru', 'beinopen.ru', 'betechno.ru', 'big-book-city.ru', \n",
    "                       'bigspravka.ru', 'bizorg.su']\n",
    "\n",
    "def get_list_of_html_files(list_of_html_files):\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # Get a list of all files in the current directory\n",
    "    file_list = os.listdir(current_directory)\n",
    "\n",
    "    # Filter and print the .html files\n",
    "    for file in file_list:\n",
    "        if file.endswith(\".html\"):\n",
    "            list_of_html_files.append(file)\n",
    "    return list_of_html_files\n",
    "\n",
    "\n",
    "\n",
    "def get_list_of_urls(list_of_html_files, list_of_urls):\n",
    "    \n",
    "    for html_file in list_of_html_files:\n",
    "            \n",
    "        with open(html_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "\n",
    "        # Создаем объект BeautifulSoup для парсинга HTML\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "        # Теперь вы можете использовать объект soup для извлечения информации из сохраненной страницы\n",
    "        # Например, найдем все элементы с тегом 'a' и выведем их атрибут 'href'\n",
    "        links = soup.find_all('a')\n",
    "        #for link in links:\n",
    "            #print(link['href'])\n",
    "\n",
    "\n",
    "        vacancies_names = soup.find_all(class_='apx8Vc qLRx3b tjvcx GvPZzd cHaqb')\n",
    "        for name in vacancies_names:\n",
    "            list_of_classes_with_urls.append(name)\n",
    "\n",
    "            #print(str(counter) + \"  \" + str(name))\n",
    "            #counter += 1\n",
    "\n",
    "\n",
    "        for element in list_of_classes_with_urls:\n",
    "            string = str(element)\n",
    "\n",
    "            # Используем регулярное выражение для поиска ссылки внутри тега 'a' с атрибутом 'href'\n",
    "            match = re.search(r'212px\">.*<span', string)\n",
    "\n",
    "            if match:\n",
    "                url = match.group(0)\n",
    "                url = url.lower()\n",
    "                if len(url) < 150 and 'avito' not in url: \n",
    "                    url1 = url\n",
    "                    url2 = url1[:-5]\n",
    "                    if '212px\">https://' in url2:\n",
    "                        url2 = url2[15:]\n",
    "                    elif '212px\">http://' in url2:\n",
    "                        url2 = url2[14:]\n",
    "                    elif 'www' in  url2:\n",
    "                        url2 = 'zal_UPA'\n",
    "                    #print(url2)\n",
    "                    list_of_urls.append(url2)\n",
    "\n",
    "        list_of_urls = list(set(list_of_urls))  \n",
    "        \n",
    "    \n",
    "    return list_of_urls\n",
    "\n",
    "def remove_cities(list_of_urls):\n",
    "\n",
    "    data1 = list_of_urls\n",
    "\n",
    "    # Process each element in the list\n",
    "    for i in range(len(data1)):\n",
    "        if data1[i].count('.') > 1:\n",
    "            # Find the index of the first dot\n",
    "            first_dot_index = data1[i].index('.')\n",
    "\n",
    "            # Remove characters up to the first dot (including the dot)\n",
    "            data1[i] = data1[i][(first_dot_index + 1):]\n",
    "\n",
    "    # Print the modified list\n",
    "    #print(data1)\n",
    "    return data1\n",
    "\n",
    "def remove_bad_sites(list_of_urls):\n",
    "    data2 = list_of_urls\n",
    "\n",
    "    bad_words = list_of_agreg_sites\n",
    "\n",
    "    # Remove elements containing bad words\n",
    "    filtered_data = [item for item in data2 if not any(word in item for word in bad_words)]\n",
    "\n",
    "    # Print the filtered list\n",
    "    #print(filtered_data)\n",
    "    return filtered_data\n",
    "\n",
    "######################################\n",
    "######################################\n",
    "\n",
    "def format_url(url):\n",
    "    # Add the protocol prefix if missing\n",
    "    if not url.startswith(\"http://\") and not url.startswith(\"https://\"):\n",
    "        url = \"http://\" + url\n",
    "    return url\n",
    "\n",
    "def process_phone_number(phone_number):\n",
    "    # Remove parentheses, spaces, and dashes from the phone number\n",
    "    processed_number = re.sub(r'[\\(\\)\\s-]', '', phone_number)\n",
    "\n",
    "    # If the number starts with 8 and the second digit is 9, replace the first digit with 7\n",
    "    if processed_number.startswith('8') and processed_number[1] == '9':\n",
    "        processed_number = '7' + processed_number[1:]\n",
    "\n",
    "    # Add 'https://wa.me/' at the beginning of the phone number\n",
    "    processed_number = 'https://wa.me/' + processed_number\n",
    "\n",
    "    return processed_number\n",
    "\n",
    "def find_information(url):\n",
    "    # Initialize the list to store the results for the given URL\n",
    "    results = []\n",
    "\n",
    "    # Format the URL with the protocol prefix if missing\n",
    "    url = format_url(url)\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url,  timeout=30)\n",
    "        print(f'started working with {url}')\n",
    "\n",
    "        # Check if the response is successful\n",
    "        if response.status_code == 200:\n",
    "            # Extract the text content from the response\n",
    "            content = response.text\n",
    "\n",
    "            # Check if 'ИНН' is present and add the result to the list\n",
    "            inn_result = re.search(r'ИНН', content)\n",
    "            results.append(inn_result.group(0) if inn_result else None)\n",
    "\n",
    "            # Check if 'ОРГН' or 'огрн' is present and add the result to the list\n",
    "            orgn_result = re.search(r'ОРГН|огрн', content)\n",
    "            results.append(orgn_result.group(0) if orgn_result else None)\n",
    "\n",
    "            # Check if 'ООО' or 'ооо' is present and add the result to the list\n",
    "            ooo_result = re.search(r'ООО|ооо', content)\n",
    "            results.append(ooo_result.group(0) if ooo_result else None)\n",
    "\n",
    "            # Check if 'общество с ограниченной ответственностью' is present and add the result to the list\n",
    "            os_result = re.search(r'общество с ограниченной ответственностью|Общество с ограниченной ответственностью|ОБЩЕСТВО С ОГРАНИЧЕННОЙ ОТВЕТСТВЕННОСТЬЮ', content)\n",
    "            results.append(os_result.group(0) if os_result else None)\n",
    "\n",
    "            # Check if 'ИП' or 'ип' is present and add the result to the list\n",
    "            ip_result = re.search(r'ИП|ип', content)\n",
    "            results.append(ip_result.group(0) if ip_result else None)\n",
    "\n",
    "            # Search for phone numbers, filter by 11 digits, and add the processed results to the list\n",
    "            phone_numbers = re.findall(r'[7|8]\\s?[9][\\d\\s()-]{9}', content)\n",
    "            processed_numbers = [process_phone_number(number) for number in phone_numbers if len(re.sub(r'[\\(\\)\\s-]', '', number)) == 11]\n",
    "            results.append(processed_numbers)\n",
    "\n",
    "        else:\n",
    "            results\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # If there is a connection error, add the error message to the results list\n",
    "        print(f\"Connection Error: {str(e)}\")\n",
    "        results.append(f\"Connection Error: {str(e)}\")\n",
    "    except requests.Timeout:\n",
    "        print(f\"Timeout occurred while downloading {url}\")\n",
    "        results.append(f\"Timeout occurred while downloading {url}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "#main\n",
    "\n",
    "name_of_industry = input('Type the name of industry')\n",
    "\n",
    "get_list_of_html_files(list_of_html_files)\n",
    "\n",
    "#\n",
    "list_of_all_urls = []\n",
    "list_of_all_urls = get_list_of_urls(list_of_html_files,list_of_urls)\n",
    "list_of_all_urls.sort()\n",
    "#print(list_of_all_urls)\n",
    "#print()\n",
    "#print()\n",
    "#print()\n",
    "#\n",
    "\n",
    "list_of_urls = remove_bad_sites(remove_cities(get_list_of_urls(list_of_html_files,list_of_urls)))\n",
    "\n",
    "result_list_of_urls = list(set(list_of_urls))\n",
    "result_list_of_urls.sort()\n",
    "print('SuperCum UAV')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List of URLs to process\n",
    "url_list = result_list_of_urls\n",
    "\n",
    "\"\"\"\n",
    "url_list = [\n",
    "    'bochka63.ru',\n",
    "    'bochki-bany.ru',\n",
    "    'bochky-bany.ru',\n",
    "    'bochky.ru',\n",
    "    'bany-bochki-24.ru',\n",
    "    'cooperwood.ru',\n",
    "    'ekaterem.ru'\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the list to store all the results\n",
    "all_results = []\n",
    "name_of_industry_list = [name_of_industry]\n",
    "\n",
    "# Process each URL\n",
    "for url in url_list:\n",
    "    url_results = find_information(url)\n",
    "\n",
    "    # Append the URL and its results to the final list\n",
    "    \n",
    "    #all_results.append([url] + name_of_industry_list + url_results)\n",
    "    all_results.append([url] + url_results)\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file_path = os.path.join(\"results_\" + name_of_industry + \".csv\")\n",
    "\n",
    "# Save the results in a CSV file with Cyrillic encoding\n",
    "with open(csv_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"URL\", \"ИНН\", \"ОРГН\", \"ООО\", \"ОС\", \"ИП\", \"Phone Numbers\"])\n",
    "    for result in all_results:\n",
    "        writer.writerow(result)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path} successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c3af4a-5dbb-430e-ba9d-00fb9ebc9280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13ead0-4b89-461f-95d4-52b9646c345b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7297ba20-19f4-4e52-bb27-ff3fb0c19b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8617654-bb4d-4fda-a62e-64b5f09ec72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67dbeb-9868-41a8-a005-bf1099f1aa78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "425a2825",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SuperCum UAV\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nprint(list_of_html_files)\\nprint('')\\nlist_of_urls.sort()\\nprint(list_of_urls)\\nprint(len(list_of_urls))\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "ask user for the name of industry\n",
    "get list of html files\n",
    "get list of urls\n",
    "remove cities from url\n",
    "remore bad sites\n",
    "\n",
    "parse main pages of websites from list of urls \n",
    "for each url find \n",
    "    telephone numbers\n",
    "    ИНН (is or not)\n",
    "    ОРГН (is or not)\n",
    "    ООО (is or not)\n",
    "    ИП (is or not)\n",
    "and for each telephone of each url create a list of  - url - ИНН - ОГРН - ООО - ИП - Telephones in a format of whatsapp link\n",
    "then save it to csv\n",
    "and then manually remove [ ] ' \" with Nitepad++\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "###main\n",
    "\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "from urllib.parse import urlparse\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "\n",
    "list_of_html_files = []\n",
    "list_of_urls = []\n",
    "list_of_classes_with_urls = []\n",
    "list_of_urls_with_www = []\n",
    "list_of_urls_without_www_of_cities = []\n",
    "result_list_of_urls = []\n",
    "\n",
    "list_of_agreg_sites = ['allix.ru','avito.ru','bazarpnz.ru','bazarsng.ru','dorus.ru','irr.ru','miltor.ru','on-doski.ru','pirpo.ru','rudos.ru','1000dosok.ru',\n",
    "                       'бесплатныеобъявления', 'pulscen','barahla','orgsinfo','uslugi','2417.pro','abtorg.ru','acoola.ru','akula.shop','allix.ru','alvengo.ru',\n",
    "                       'avada.shop','avelango.com','avito.ru','barahla.net','bazarpnz.ru','bazarsng.ru','biz-market.ru','bulboard.ru','cmlt.ru','deilo.ru',\n",
    "                       'delonet.ru','diesel.world','dorus.ru','doska.ru','doski.ru','flagma.ru','irr.ru','ivik.ru','kaluga.sale','kupiprodai.ru','megadoski.ru',\n",
    "                       'miltor.ru','moyareklama.ru','nadoske.info','on-doski.ru','pirpo.ru','prodam-rf.ru','reslando.ru','roodls.net','rudos.ru','sindom.ru',\n",
    "                       'trasto.ru','youla.ru','объявления.рф','объявления.рф'  '1000dosok.ru', '2gis', 'EDC.SALE', 'aport', 'asktel.ru', 'avito', 'barahla.net', \n",
    "                       'blizko', 'bulboard', 'castorama', 'cataloxy', 'farpost.ru', 'fis.ru', 'gdeprosto', 'google', 'hh', 'irr.ru', 'kupiprodai', 'kupiprodai.ru', \n",
    "                       'livingjoy', 'mail.ru', 'mk.ru', 'moyareklama', 'nashaspravka', 'okigo.ru', 'optlist', 'otzovik', 'ozon', 'poisk', 'productcenter', \n",
    "                       'propartner', 'regmarkets', 'rydo.ru', 'skidkom', 'spark-interfax', 'spravker', 'stroyportal', 'tdsot.ru', 'tuttovary', 'vdoskusvoi', \n",
    "                       'vk.com', 'vseinstrumenti', 'wb', 'wildberries', 'yandex', 'yell.ru', 'youla', 'zoon']\n",
    "\n",
    "def get_list_of_html_files(list_of_html_files):\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # Get a list of all files in the current directory\n",
    "    file_list = os.listdir(current_directory)\n",
    "\n",
    "    # Filter and print the .html files\n",
    "    for file in file_list:\n",
    "        if file.endswith(\".html\"):\n",
    "            list_of_html_files.append(file)\n",
    "    return list_of_html_files\n",
    "\n",
    "\n",
    "\n",
    "def get_list_of_urls(list_of_html_files, list_of_urls):\n",
    "    \n",
    "    for html_file in list_of_html_files:\n",
    "            \n",
    "        with open(html_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "\n",
    "        # Создаем объект BeautifulSoup для парсинга HTML\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "        # Теперь вы можете использовать объект soup для извлечения информации из сохраненной страницы\n",
    "        # Например, найдем все элементы с тегом 'a' и выведем их атрибут 'href'\n",
    "        links = soup.find_all('a')\n",
    "        #for link in links:\n",
    "            #print(link['href'])\n",
    "\n",
    "\n",
    "        vacancies_names = soup.find_all(class_='apx8Vc qLRx3b tjvcx GvPZzd cHaqb')\n",
    "        for name in vacancies_names:\n",
    "            list_of_classes_with_urls.append(name)\n",
    "\n",
    "            #print(str(counter) + \"  \" + str(name))\n",
    "            #counter += 1\n",
    "\n",
    "\n",
    "        for element in list_of_classes_with_urls:\n",
    "            string = str(element)\n",
    "\n",
    "            # Используем регулярное выражение для поиска ссылки внутри тега 'a' с атрибутом 'href'\n",
    "            match = re.search(r'212px\">.*<span', string)\n",
    "\n",
    "            if match:\n",
    "                url = match.group(0)\n",
    "                url = url.lower()\n",
    "                if len(url) < 150 and 'avito' not in url: \n",
    "                    url1 = url\n",
    "                    url2 = url1[:-5]\n",
    "                    if '212px\">https://' in url2:\n",
    "                        url2 = url2[15:]\n",
    "                    elif '212px\">http://' in url2:\n",
    "                        url2 = url2[14:]\n",
    "                    elif 'www' in  url2:\n",
    "                        url2 = 'zalupa'\n",
    "                    #print(url2)\n",
    "                    list_of_urls.append(url2)\n",
    "\n",
    "        list_of_urls = list(set(list_of_urls))  \n",
    "        \n",
    "    \n",
    "    return list_of_urls\n",
    "\n",
    "def remove_cities(list_of_urls):\n",
    "\n",
    "    data1 = list_of_urls\n",
    "\n",
    "    # Process each element in the list\n",
    "    for i in range(len(data1)):\n",
    "        if data1[i].count('.') > 1:\n",
    "            # Find the index of the first dot\n",
    "            first_dot_index = data1[i].index('.')\n",
    "\n",
    "            # Remove characters up to the first dot (including the dot)\n",
    "            data1[i] = data1[i][(first_dot_index + 1):]\n",
    "\n",
    "    # Print the modified list\n",
    "    #print(data1)\n",
    "    return data1\n",
    "\n",
    "def remove_bad_sites(list_of_urls):\n",
    "    data2 = list_of_urls\n",
    "\n",
    "    bad_words = list_of_agreg_sites\n",
    "\n",
    "    # Remove elements containing bad words\n",
    "    filtered_data = [item for item in data2 if not any(word in item for word in bad_words)]\n",
    "\n",
    "    # Print the filtered list\n",
    "    #print(filtered_data)\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "\n",
    "#main\n",
    "get_list_of_html_files(list_of_html_files)\n",
    "\n",
    "#\n",
    "list_of_all_urls = []\n",
    "list_of_all_urls = get_list_of_urls(list_of_html_files,list_of_urls)\n",
    "list_of_all_urls.sort()\n",
    "#print(list_of_all_urls)\n",
    "#print()\n",
    "#print()\n",
    "#print()\n",
    "#\n",
    "\n",
    "list_of_urls = remove_bad_sites(remove_cities(get_list_of_urls(list_of_html_files,list_of_urls)))\n",
    "\n",
    "result_list_of_urls = list(set(list_of_urls))\n",
    "result_list_of_urls.sort()\n",
    "print('SuperCum UAV')\n",
    "\n",
    "\"\"\"\n",
    "print(list_of_html_files)\n",
    "print('')\n",
    "list_of_urls.sort()\n",
    "print(list_of_urls)\n",
    "print(len(list_of_urls))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d6d2c95-d96a-4e64-9f8c-9f0891db9f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "from urllib.parse import urlparse\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "def format_url(url):\n",
    "    # Add the protocol prefix if missing\n",
    "    if not url.startswith(\"http://\") and not url.startswith(\"https://\"):\n",
    "        url = \"http://\" + url\n",
    "    return url\n",
    "\n",
    "def process_phone_number(phone_number):\n",
    "    # Remove parentheses, spaces, and dashes from the phone number\n",
    "    processed_number = re.sub(r'[\\(\\)\\s-]', '', phone_number)\n",
    "\n",
    "    # If the number starts with 8 and the second digit is 9, replace the first digit with 7\n",
    "    if processed_number.startswith('8') and processed_number[1] == '9':\n",
    "        processed_number = '7' + processed_number[1:]\n",
    "\n",
    "    # Add 'https://wa.me/' at the beginning of the phone number\n",
    "    processed_number = 'https://wa.me/' + processed_number\n",
    "\n",
    "    return processed_number\n",
    "\n",
    "def find_information(url):\n",
    "    # Initialize the list to store the results for the given URL\n",
    "    results = []\n",
    "\n",
    "    # Format the URL with the protocol prefix if missing\n",
    "    url = format_url(url)\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the response is successful\n",
    "        if response.status_code == 200:\n",
    "            # Extract the text content from the response\n",
    "            content = response.text\n",
    "\n",
    "            # Check if 'ИНН' is present and add the result to the list\n",
    "            inn_result = re.search(r'ИНН', content)\n",
    "            results.append(inn_result.group(0) if inn_result else None)\n",
    "\n",
    "            # Check if 'ОРГН' or 'огрн' is present and add the result to the list\n",
    "            orgn_result = re.search(r'ОРГН|огрн', content)\n",
    "            results.append(orgn_result.group(0) if orgn_result else None)\n",
    "\n",
    "            # Check if 'ООО' or 'ооо' is present and add the result to the list\n",
    "            ooo_result = re.search(r'ООО|ооо', content)\n",
    "            results.append(ooo_result.group(0) if ooo_result else None)\n",
    "\n",
    "            # Check if 'общество с ограниченной ответственностью' is present and add the result to the list\n",
    "            os_result = re.search(r'общество с ограниченной ответственностью|Общество с ограниченной ответственностью|ОБЩЕСТВО С ОГРАНИЧЕННОЙ ОТВЕТСТВЕННОСТЬЮ', content)\n",
    "            results.append(os_result.group(0) if os_result else None)\n",
    "\n",
    "            # Check if 'ИП' or 'ип' is present and add the result to the list\n",
    "            ip_result = re.search(r'ИП|ип', content)\n",
    "            results.append(ip_result.group(0) if ip_result else None)\n",
    "\n",
    "            # Search for phone numbers, filter by 11 digits, and add the processed results to the list\n",
    "            phone_numbers = re.findall(r'[7|8]\\s?[9][\\d\\s()-]{9}', content)\n",
    "            processed_numbers = [process_phone_number(number) for number in phone_numbers if len(re.sub(r'[\\(\\)\\s-]', '', number)) == 11]\n",
    "            results.append(processed_numbers)\n",
    "\n",
    "        else:\n",
    "            results\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # If there is a connection error, add the error message to the results list\n",
    "        results.append(f\"Connection Error: {str(e)}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# List of URLs to process\n",
    "url_list = result_list_of_urls\n",
    "\n",
    "\"\"\"\n",
    "url_list = [\n",
    "    'bochka63.ru',\n",
    "    'bochki-bany.ru',\n",
    "    'bochky-bany.ru',\n",
    "    'bochky.ru',\n",
    "    'bany-bochki-24.ru',\n",
    "    'cooperwood.ru',\n",
    "    'ekaterem.ru'\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the list to store all the results\n",
    "all_results = []\n",
    "\n",
    "# Process each URL\n",
    "for url in url_list:\n",
    "    url_results = find_information(url)\n",
    "\n",
    "    # Append the URL and its results to the final list\n",
    "    all_results.append([url] + url_results)\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file_path = os.path.join(\"results.csv\")\n",
    "\n",
    "# Save the results in a CSV file with Cyrillic encoding\n",
    "with open(csv_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"URL\", \"ИНН\", \"ОРГН\", \"ООО\", \"ОС\", \"ИП\", \"Phone Numbers\"])\n",
    "    for result in all_results:\n",
    "        writer.writerow(result)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path} successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79f784e-cf2b-401f-9059-87350462b08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d7fe96-5eba-4789-b85c-48535d263581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc9f35-21ff-4ea2-bec4-be12fb9ba2db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64424ec2-cb79-405d-80d4-26ff25e5d56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e8b60c1-7bed-47c1-b14a-3bb274a7f787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1rnd.ru',\n",
       " 'activlife.shop',\n",
       " 'alfalesstroy.ru',\n",
       " 'arh-besedki.ru',\n",
       " 'banbochka.ru',\n",
       " 'bani-bo4ki.ru',\n",
       " 'bani.ru.com',\n",
       " 'banibochkino.ru',\n",
       " 'banidom.ru',\n",
       " 'baniglushakova.ru',\n",
       " 'bany-bochki-24.ru',\n",
       " 'banya-bochka.su',\n",
       " 'banyaprosto.ru',\n",
       " 'bochka63.ru',\n",
       " 'bochki-bany.ru',\n",
       " 'bochky-bany.ru',\n",
       " 'bochky.ru',\n",
       " 'bytovki-rf.ru',\n",
       " 'canadskaya-izba.ru',\n",
       " 'cmz-dobrostroy.ru',\n",
       " 'cooperwood.ru',\n",
       " 'dkedra.ru',\n",
       " 'domaa.ru',\n",
       " 'dompechei.ru',\n",
       " 'ekaterem.ru',\n",
       " 'emkosti-online.ru',\n",
       " 'eurobesedka.ru',\n",
       " 'fitobochki.ru',\n",
       " 'fitorodnik.ru',\n",
       " 'goodster.ru',\n",
       " 'gorod-masterov.com',\n",
       " 'grill-landia.ru',\n",
       " 'grilland.ru',\n",
       " 'ivdrevdom.ru',\n",
       " 'jsprav.ru',\n",
       " 'karkas.shop',\n",
       " 'kedrprom.ru',\n",
       " 'kedrvetv.ru',\n",
       " 'klen-msk.ru',\n",
       " 'kskcivil.ru',\n",
       " 'kuplusrazu.ru',\n",
       " 'lesholdinggroup.ru',\n",
       " 'lesstroy.net',\n",
       " 'megastroy.com',\n",
       " 'mrmag.ru',\n",
       " 'noodlemagazine.com',\n",
       " 'ochag21.ru',\n",
       " 'promindex.ru',\n",
       " 'regtorg.ru',\n",
       " 'roskedr.ru',\n",
       " 'rospar.ru',\n",
       " 'rstk.net',\n",
       " 'rusprofile.ru',\n",
       " 'saintbor.ru',\n",
       " 'saray.ru',\n",
       " 'saunadoma.com',\n",
       " 'saunapeterburg.ru',\n",
       " 'saunayug.ru',\n",
       " 'sgsthermo.com',\n",
       " 'sk-evrodom.ru',\n",
       " 'skglushakov.ru',\n",
       " 'srub.store',\n",
       " 'steamwood.pro',\n",
       " 'stroylandiya.ru',\n",
       " 'stroymir53.ru',\n",
       " 'tairai.ru',\n",
       " 'tara-51.ru',\n",
       " 'tara.ru',\n",
       " 'tara32.ru',\n",
       " 'teplodina.ru',\n",
       " 'teplokedra.ru',\n",
       " 'teplozhar.ru',\n",
       " 'tk31.su',\n",
       " 'tuzlist.ru',\n",
       " 'vuoksa-piers.ru',\n",
       " 'wood-water.ru',\n",
       " 'бани-бочки.рф',\n",
       " 'банибочки.рф',\n",
       " 'банибочки55.рф',\n",
       " 'банибочкикупить.рф',\n",
       " 'баня-викинг.рф',\n",
       " 'бассейны-спа.рф',\n",
       " 'вбрусе.рф',\n",
       " 'империябань.рф',\n",
       " 'клен-влд.рф',\n",
       " 'проект-бани.рф',\n",
       " 'термо-доска.рф',\n",
       " 'царский-стиль.рф',\n",
       " 'чудо-баня.рф',\n",
       " 'чудобаня.рф',\n",
       " 'югабсолютстрой.рф']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list_of_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c4325c-f17b-46b1-855e-709248e81b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e5f7b8c-c0ff-4dce-a8a1-ad30d0a39a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConnectionError occurred while accessing http://activlife.shop: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Error 403 occurred while accessing http://bany-bochki-24.ru\n",
      "Error 403 occurred while accessing http://cooperwood.ru\n",
      "Error 404 occurred while accessing http://ekaterem.ru\n",
      "ConnectionError occurred while accessing http://fitorodnik.ru: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Error 403 occurred while accessing http://mrmag.ru\n",
      "Error 403 occurred while accessing http://ochag21.ru\n",
      "Error 403 occurred while accessing http://regtorg.ru\n",
      "ConnectionError occurred while accessing http://rospar.ru: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Error 403 occurred while accessing http://rstk.net\n",
      "Error 403 occurred while accessing http://rusprofile.ru\n",
      "ConnectionError occurred while accessing http://saunadoma.com: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Error 520 occurred while accessing http://stroylandiya.ru\n",
      "ConnectionError occurred while accessing http://клен-влд.рф: HTTPSConnectionPool(host='xn----dtbgbygdn.xn--p1ai', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))\n",
      "Results saved to results.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "from urllib.parse import urlparse\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "def format_url(url):\n",
    "    # Add the protocol prefix if missing\n",
    "    if not url.startswith(\"http://\") and not url.startswith(\"https://\"):\n",
    "        url = \"http://\" + url\n",
    "    return url\n",
    "\n",
    "def find_information(url):\n",
    "    # Initialize the list to store the results for the given URL\n",
    "    results = []\n",
    "\n",
    "    # Format the URL with the protocol prefix if missing\n",
    "    url = format_url(url)\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the response is successful\n",
    "        if response.status_code == 200:\n",
    "            # Extract the text content from the response\n",
    "            content = response.text\n",
    "\n",
    "            # Check if 'ИНН' is present and add the result to the list\n",
    "            inn_result = re.search(r'ИНН', content)\n",
    "            results.append(inn_result.group(0) if inn_result else None)\n",
    "\n",
    "            # Check if 'ОРГН' or 'огрн' is present and add the result to the list\n",
    "            orgn_result = re.search(r'ОРГН|огрн', content)\n",
    "            results.append(orgn_result.group(0) if orgn_result else None)\n",
    "\n",
    "            # Check if 'ООО' or 'ооо' is present and add the result to the list\n",
    "            ooo_result = re.search(r'ООО|ооо', content)\n",
    "            results.append(ooo_result.group(0) if ooo_result else None)\n",
    "\n",
    "            # Check if 'общество с ограниченной ответственностью' is present and add the result to the list\n",
    "            os_result = re.search(r'общество с ограниченной ответственностью|Общество с ограниченной ответственностью|ОБЩЕСТВО С ОГРАНИЧЕННОЙ ОТВЕТСТВЕННОСТЬЮ', content)\n",
    "            results.append(os_result.group(0) if os_result else None)\n",
    "\n",
    "            # Check if 'ИП' or 'ип' is present and add the result to the list\n",
    "            ip_result = re.search(r'ИП|ип', content)\n",
    "            results.append(ip_result.group(0) if ip_result else None)\n",
    "\n",
    "            # Search for phone numbers and add the results to the list\n",
    "            phone_numbers = re.findall(r'[+7|8]\\s?\\d{3}[\\s-]?\\d{3}[\\s-]?\\d{2}[\\s-]?\\d{2}', content)\n",
    "            results.append(phone_numbers)\n",
    "\n",
    "        else:\n",
    "            print(f\"Error {response.status_code} occurred while accessing {url}\")\n",
    "\n",
    "    except ConnectionError as e:\n",
    "        print(f\"ConnectionError occurred while accessing {url}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Specify the list of URLs\n",
    "\n",
    "url_list = result_list_of_urls\n",
    "\"\"\"\n",
    "url_list = [\n",
    "    \"example.com\",\n",
    "    \"google.com\",\n",
    "    \"github.com\"\n",
    "]\"\"\"\n",
    "\n",
    "# List to store the results for each URL\n",
    "all_results = []\n",
    "\n",
    "# Process each URL\n",
    "for url in url_list:\n",
    "    # Find information for the current URL\n",
    "    url_results = find_information(url)\n",
    "\n",
    "    # Add the URL and its results to the overall results list\n",
    "    all_results.append([url] + url_results)\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file_path = os.path.join(\"results.csv\")\n",
    "\n",
    "# Save the results in a CSV file with Cyrillic encoding\n",
    "with open(csv_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"URL\", \"ИНН\", \"ОРГН\", \"ООО\", \"ОС\", \"ИП\", \"Phone Numbers\"])\n",
    "    writer.writerows(all_results)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba2d1da-ca4f-4ef9-a3ae-eb109cc451ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e4b83d3-d0a2-4e41-b974-8f21c43b2c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1rnd.ru',\n",
       " 'activlife.shop',\n",
       " 'alfalesstroy.ru',\n",
       " 'arh-besedki.ru',\n",
       " 'banbochka.ru',\n",
       " 'bani-bo4ki.ru',\n",
       " 'bani.ru.com',\n",
       " 'banibochkino.ru',\n",
       " 'banidom.ru',\n",
       " 'baniglushakova.ru',\n",
       " 'bany-bochki-24.ru',\n",
       " 'banya-bochka.su',\n",
       " 'banyaprosto.ru',\n",
       " 'bochka63.ru',\n",
       " 'bochki-bany.ru',\n",
       " 'bochky-bany.ru',\n",
       " 'bochky.ru',\n",
       " 'bytovki-rf.ru',\n",
       " 'canadskaya-izba.ru',\n",
       " 'cmz-dobrostroy.ru',\n",
       " 'cooperwood.ru',\n",
       " 'dkedra.ru',\n",
       " 'domaa.ru',\n",
       " 'dompechei.ru',\n",
       " 'ekaterem.ru',\n",
       " 'emkosti-online.ru',\n",
       " 'eurobesedka.ru',\n",
       " 'fitobochki.ru',\n",
       " 'fitorodnik.ru',\n",
       " 'goodster.ru',\n",
       " 'gorod-masterov.com',\n",
       " 'grill-landia.ru',\n",
       " 'grilland.ru',\n",
       " 'ivdrevdom.ru',\n",
       " 'jsprav.ru',\n",
       " 'karkas.shop',\n",
       " 'kedrprom.ru',\n",
       " 'kedrvetv.ru',\n",
       " 'klen-msk.ru',\n",
       " 'kskcivil.ru',\n",
       " 'kuplusrazu.ru',\n",
       " 'lesholdinggroup.ru',\n",
       " 'lesstroy.net',\n",
       " 'megastroy.com',\n",
       " 'mrmag.ru',\n",
       " 'noodlemagazine.com',\n",
       " 'ochag21.ru',\n",
       " 'promindex.ru',\n",
       " 'regtorg.ru',\n",
       " 'roskedr.ru',\n",
       " 'rospar.ru',\n",
       " 'rstk.net',\n",
       " 'rusprofile.ru',\n",
       " 'saintbor.ru',\n",
       " 'saray.ru',\n",
       " 'saunadoma.com',\n",
       " 'saunapeterburg.ru',\n",
       " 'saunayug.ru',\n",
       " 'sgsthermo.com',\n",
       " 'sk-evrodom.ru',\n",
       " 'skglushakov.ru',\n",
       " 'srub.store',\n",
       " 'steamwood.pro',\n",
       " 'stroylandiya.ru',\n",
       " 'stroymir53.ru',\n",
       " 'tairai.ru',\n",
       " 'tara-51.ru',\n",
       " 'tara.ru',\n",
       " 'tara32.ru',\n",
       " 'teplodina.ru',\n",
       " 'teplokedra.ru',\n",
       " 'teplozhar.ru',\n",
       " 'tk31.su',\n",
       " 'tuzlist.ru',\n",
       " 'vuoksa-piers.ru',\n",
       " 'wood-water.ru',\n",
       " 'бани-бочки.рф',\n",
       " 'банибочки.рф',\n",
       " 'банибочки55.рф',\n",
       " 'банибочкикупить.рф',\n",
       " 'баня-викинг.рф',\n",
       " 'бассейны-спа.рф',\n",
       " 'вбрусе.рф',\n",
       " 'империябань.рф',\n",
       " 'клен-влд.рф',\n",
       " 'проект-бани.рф',\n",
       " 'термо-доска.рф',\n",
       " 'царский-стиль.рф',\n",
       " 'чудо-баня.рф',\n",
       " 'чудобаня.рф',\n",
       " 'югабсолютстрой.рф']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list_of_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9ad35dc1-dcc5-4e73-8cf0-1a5c98140991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_list_of_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2669fbab-687d-4f85-bf1b-1480a9ad536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_webpages_and_make_list_of_success_and_failed(name_of_industry, input_list_of_urls):    \n",
    "    url_with_http_www = ''\n",
    "    list_of_downloaded_pages = []\n",
    "\n",
    "    def download_webpage(url, output_folder):\n",
    "        # Create the output folder if it doesn't exist\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Extract the domain name from the URL\n",
    "        parsed_url = urlparse(url)\n",
    "        domain_name = parsed_url.netloc\n",
    "\n",
    "        # Save the webpage to a file with the domain name as the filename\n",
    "        output_path = os.path.join(output_folder, f\"{domain_name}.html\")\n",
    "        with open(output_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "        print(f\"Website downloaded and saved to: {output_path}\")\n",
    "\n",
    "    # Specify the list of URLs and output folder\n",
    "    list_of_urls = input_list_of_urls\n",
    "    output_folder = \"downloaded_main_pages \" + str(name_of_industry)\n",
    "\n",
    "    # Download webpages for each URL in the list\n",
    "    for url in list_of_urls:\n",
    "        url_with_http_www = 'https://' + str(url)\n",
    "        try:\n",
    "            download_webpage(url_with_http_www, output_folder)\n",
    "            list_of_downloaded_pages.append(str(url_with_http_www) + '__sussecc')\n",
    "        except:\n",
    "            list_of_downloaded_pages.append(str(url_with_http_www) + '__fail')\n",
    "\n",
    "    return list_of_downloaded_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a5bff-5f7a-4338-a317-68cb20539741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type the name of industry banya_bochka\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\1rnd.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\arh-besedki.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\banbochka.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\bani-bo4ki.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\bani.ru.com.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\banidom.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\baniglushakova.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\bany-bochki-24.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\banya-bochka.su.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\banyaprosto.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\bochka63.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\bochki-bany.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\bochky-bany.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\bochky.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\bytovki-rf.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\canadskaya-izba.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\cmz-dobrostroy.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\cooperwood.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\dkedra.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\domaa.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\dompechei.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\ekaterem.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\emkosti-online.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\eurobesedka.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\fitobochki.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\goodster.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\gorod-masterov.com.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\grill-landia.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\grilland.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\ivdrevdom.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\jsprav.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\kedrprom.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\kedrvetv.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\klen-msk.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\kskcivil.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\kuplusrazu.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\lesholdinggroup.ru.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\lesstroy.net.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\megastroy.com.html\n",
      "Website downloaded and saved to: downloaded_main_pages banya_bochka\\mrmag.ru.html\n"
     ]
    }
   ],
   "source": [
    "name_of_industry = ''\n",
    "name_of_industry = input('Type the name of industry')\n",
    "\n",
    "download_webpages(name_of_industry, result_list_of_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ba3788-d628-4cfc-8c73-dadabd252fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
